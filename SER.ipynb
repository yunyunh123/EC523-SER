{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5cc2ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 11:00:24 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:18:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0             43W /  300W |       0MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0             44W /  300W |       0MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2-16GB           On  |   00000000:86:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0             43W /  300W |       0MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2-16GB           On  |   00000000:AF:00.0 Off |                    0 |\r\n",
      "| N/A   43C    P0             44W /  300W |       0MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# check GPU status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "591febde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q librosa\n",
    "!pip install -q opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a716832",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd065322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/projectnb/dl523/students/eburhan/EC523-SER', '', '/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.10/site-packages', '/share/pkg.8/python3/3.10.12/install/lib/python310.zip', '/share/pkg.8/python3/3.10.12/install/lib/python3.10', '/share/pkg.8/python3/3.10.12/install/lib/python3.10/lib-dynload', '/usr4/ec500kb/eburhan/.local/lib/python3.10/site-packages', '/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is an Ellen issue. For some reason I keep on getting path issue on the modules.\"\"\"\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "PATH = \"/projectnb/ec500kb/students/eburhan/Project/venvs/mynewenv/lib/python3.10/site-packages\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2711227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import math\n",
    "import random\n",
    "import IPython\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import librosa\n",
    "import torchaudio\n",
    "# import torchvision.transforms as transforms\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torchaudio import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio.utils import download_asset\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from torchvision import datasets\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from torchsummary import summary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7694864",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf14d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.1553e-05, -3.0518e-04, -7.9346e-04,  ..., -1.2207e-03,\n",
      "        -1.4343e-03, -1.5259e-03])\n",
      "anxiety\n",
      "                                                filename speaker_n intensity  \\\n",
      "0      ./datasets/berlin-database-of-emotional-speech...        16        NA   \n",
      "1      ./datasets/berlin-database-of-emotional-speech...        10        NA   \n",
      "2      ./datasets/berlin-database-of-emotional-speech...        16        NA   \n",
      "3      ./datasets/berlin-database-of-emotional-speech...        16        NA   \n",
      "4      ./datasets/berlin-database-of-emotional-speech...        14        NA   \n",
      "...                                                  ...       ...       ...   \n",
      "15692  ./datasets/shemo-persian-speech-emotion-detect...        56        NA   \n",
      "15693  ./datasets/shemo-persian-speech-emotion-detect...        12        NA   \n",
      "15694  ./datasets/shemo-persian-speech-emotion-detect...        12        NA   \n",
      "15695  ./datasets/shemo-persian-speech-emotion-detect...        04        NA   \n",
      "15696  ./datasets/shemo-persian-speech-emotion-detect...        12        NA   \n",
      "\n",
      "       emotion version language database  \n",
      "0      anxiety       a   german    emodb  \n",
      "1        bored       b   german    emodb  \n",
      "2        bored       a   german    emodb  \n",
      "3      neutral       b   german    emodb  \n",
      "4        bored       a   german    emodb  \n",
      "...        ...     ...      ...      ...  \n",
      "15692  neutral       5  persian    shemo  \n",
      "15693  neutral      70  persian    shemo  \n",
      "15694    anger       9  persian    shemo  \n",
      "15695  neutral      44  persian    shemo  \n",
      "15696    anger      68  persian    shemo  \n",
      "\n",
      "[15697 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset import download_datasets, SpeechEmotionDataset, get_dataset_info\n",
    "\n",
    "# Specify the directory you want the datasets to be contained in\n",
    "dataset_dir = './datasets'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "####\n",
    "# Only needed to run this code once\n",
    "####\n",
    "## Download a single dataset\n",
    "# download_datasets(dataset_dir, dname=\"emodb\")\n",
    "#\n",
    "## Download the rest of the datasets available\n",
    "# download_datasets(dataset_dir)\n",
    "\n",
    "\n",
    "# Acquire info on datasets (those that have functions to get data for)\n",
    "df = get_dataset_info(dataset_dir)\n",
    "\n",
    "# Make into a Dataset object that a pytorch optimizer can use\n",
    "# Can optionally specify a sampling rate for all audio files to be in\n",
    "trainset = SpeechEmotionDataset(df, fs=16000)\n",
    "\n",
    "# Check it works\n",
    "dataiter = iter(trainset)\n",
    "data, label = next(dataiter)\n",
    "print(data)\n",
    "print(label)\n",
    "print(df) # columns are: filename, speaker_n, intensity, emotion, version, language, database \n",
    "\n",
    "# # Put into a dataloader\n",
    "# trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9c49ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        ./datasets/berlin-database-of-emotional-speech...\n",
      "1        ./datasets/berlin-database-of-emotional-speech...\n",
      "2        ./datasets/berlin-database-of-emotional-speech...\n",
      "3        ./datasets/berlin-database-of-emotional-speech...\n",
      "4        ./datasets/berlin-database-of-emotional-speech...\n",
      "                               ...                        \n",
      "15692    ./datasets/shemo-persian-speech-emotion-detect...\n",
      "15693    ./datasets/shemo-persian-speech-emotion-detect...\n",
      "15694    ./datasets/shemo-persian-speech-emotion-detect...\n",
      "15695    ./datasets/shemo-persian-speech-emotion-detect...\n",
      "15696    ./datasets/shemo-persian-speech-emotion-detect...\n",
      "Name: filename, Length: 15697, dtype: object \n",
      "\n",
      "Emotions:  ['anxiety' 'bored' 'neutral' 'disgust' 'anger' 'sadness' 'happy'\n",
      " 'surprise' 'fear' 'calm'] \n",
      "\n",
      "Num of classes:  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Data\n",
    "X = df['filename']\n",
    "print(X, \"\\n\")\n",
    "\n",
    "### Label encoding features\n",
    "print(\"Emotions: \", df['emotion'].unique(), \"\\n\")\n",
    "print(\"Num of classes: \", len(df['emotion'].unique()), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be007fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe:  (15697, 7) \n",
      "\n",
      "Column headers of dataframe:  Index(['filename', 'speaker_n', 'intensity', 'emotion', 'version', 'language',\n",
      "       'database'],\n",
      "      dtype='object') \n",
      "\n",
      "Integer encoding:  [1 2 2 ... 0 7 0] \n",
      "\n",
      "One hot encoding:  [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "Current dataframe: \n",
      "                                                filename  \\\n",
      "0      ./datasets/berlin-database-of-emotional-speech...   \n",
      "1      ./datasets/berlin-database-of-emotional-speech...   \n",
      "2      ./datasets/berlin-database-of-emotional-speech...   \n",
      "3      ./datasets/berlin-database-of-emotional-speech...   \n",
      "4      ./datasets/berlin-database-of-emotional-speech...   \n",
      "...                                                  ...   \n",
      "15692  ./datasets/shemo-persian-speech-emotion-detect...   \n",
      "15693  ./datasets/shemo-persian-speech-emotion-detect...   \n",
      "15694  ./datasets/shemo-persian-speech-emotion-detect...   \n",
      "15695  ./datasets/shemo-persian-speech-emotion-detect...   \n",
      "15696  ./datasets/shemo-persian-speech-emotion-detect...   \n",
      "\n",
      "                                          emotion_onehot  \n",
      "0      [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "4      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "...                                                  ...  \n",
      "15692  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "15693  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "15694  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "15695  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "15696  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "\n",
      "[15697 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of dataframe: \", df.shape, \"\\n\")\n",
    "\n",
    "column_headers = df.columns\n",
    "print(\"Column headers of dataframe: \", column_headers, \"\\n\")\n",
    "\n",
    "df_subset = df[['filename', 'emotion']]\n",
    "\n",
    "# Perform one-hot encoding on 'emotion'\n",
    "# Integer encoding\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoding = label_encoder.fit_transform(df_subset['emotion'])\n",
    "print('Integer encoding: ', integer_encoding, \"\\n\")\n",
    "\n",
    "# Binary encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoding = integer_encoding.reshape(-1, 1)\n",
    "one_hot_encoding = one_hot_encoder.fit_transform(integer_encoding)\n",
    "print('One hot encoding: ', one_hot_encoding, \"\\n\")\n",
    "\n",
    "# One-hot encoding to DataFrame\n",
    "one_hot_df = pd.DataFrame(one_hot_encoding, columns=label_encoder.classes_)\n",
    "result_df = pd.concat([df_subset['filename'], one_hot_df], axis=1)\n",
    "\n",
    "# Combining emotions into one array for each file name (drop individual one-hot encoded columns)\n",
    "result_df['emotion_onehot'] = result_df.iloc[:, 1:].values.tolist()\n",
    "result_df.drop(result_df.columns[1:-1], axis=1, inplace=True)\n",
    "\n",
    "print(\"Current dataframe: \")\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa2eab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from: https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5\n",
    "\"\"\"\n",
    "class audio_preprocessing():\n",
    "    def read_file(file):\n",
    "        signal, sample_rate = torchaudio.load(file)\n",
    "        \n",
    "        return (signal, sample_rate)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Standardize number of audio channels\n",
    "    # ---------------------------\n",
    "    def set_num_channel(audio, desired_num_channel):\n",
    "        signal, sample_rate = audio\n",
    "        \n",
    "        if(signal.shape[0] == desired_num_channel): # No change\n",
    "            return audio\n",
    "        \n",
    "        if(desired_num_channel == 1): # Converting stereo to mono\n",
    "            new_signal = signal[:1, :]\n",
    "        else:\n",
    "            new_signal = torch.cat([signal, signal])\n",
    "            \n",
    "        return ((new_signal, sample_rate))\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Standardize sampling rate\n",
    "    # ---------------------------    \n",
    "    def set_sampling_rate(audio, new_sr):\n",
    "        signal, sampling_rate = audio\n",
    "        \n",
    "        if(sampling_rate == new_sr):\n",
    "            return audio\n",
    "        \n",
    "        num_channels = signal.shape[0]\n",
    "        \n",
    "        # Resampling first channel\n",
    "        channel_1 = torchaudio.transforms.Resample(sampling_rate, new_sr)(signal[:1,:])\n",
    "        \n",
    "        if (num_channels > 1):\n",
    "            # Resample the second channel and merge both channels\n",
    "            channel_2 = torchaudio.transforms.Resample(sampling_rate, new_sr)(signal[1:,:])\n",
    "            resample = torch.cat([channel_1, channel_2])\n",
    "        else:\n",
    "            resample = channel_1\n",
    "\n",
    "        return ((resample, new_sr))\n",
    "    \n",
    "    \n",
    "    # ----------------------------\n",
    "    # Standardize length of audio samples\n",
    "    # max_ms = milliseconds\n",
    "    # --------------------------- \n",
    "    def standardize_audio_length(audio, max_ms):\n",
    "        signal, sampling_rate = audio\n",
    "        num_rows, signal_len = signal.shape\n",
    "        max_len = sampling_rate//1000 * max_ms\n",
    "\n",
    "        if (signal_len > max_len):\n",
    "          # Truncate the signal to the given length\n",
    "          signal = signal[:,:max_len]\n",
    "\n",
    "        elif (signal_len < max_len):\n",
    "            # Length of padding to add at the beginning and end of the signal\n",
    "            pad_begin_len = random.randint(0, max_len - signal_len)\n",
    "            pad_end_len = max_len - signal_len - pad_begin_len\n",
    "\n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "            signal = torch.cat((pad_begin, signal, pad_end), 1)\n",
    "      \n",
    "        return (signal, sampling_rate)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Shifts the signal to the left or right by some percent. Values at the end\n",
    "    # are 'wrapped around' to the start of the transformed signal.\n",
    "    # ----------------------------\n",
    "    def time_shift(audio, shift_limit): # Not sure if we need this\n",
    "        signal, sample_rate = audio\n",
    "        _, signal_len = signal.shape\n",
    "        shift_amt = int(random.random() * shift_limit * signal_len)\n",
    "        \n",
    "        return (signal.roll(shift_amt), sample_rate)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Generate a Spectrogram\n",
    "    # ----------------------------\n",
    "    def generate_mfcc_spectrogram(audio, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        signal,sample_rate = audio\n",
    "        top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(signal)\n",
    "\n",
    "        # Convert to decibels\n",
    "        spec = torchaudio.transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        \n",
    "        return (spec)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "    # overfitting and to help the model generalise better. The masked sections are\n",
    "    # replaced with the mean value.\n",
    "    # ----------------------------\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        \n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = torchaudio.transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948fc367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating Data Loader\n",
    "\"\"\"\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.duration = 4000\n",
    "        self.sr = 16000\n",
    "        self.channel = 1\n",
    "        self.shift_pct = 0.4\n",
    "            \n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "\n",
    "    # ----------------------------\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Extracting filename and one-hot encoded emotions\n",
    "        filename = self.df.loc[idx, 'filename']\n",
    "        emotion_onehot = torch.tensor(self.df.loc[idx, 'emotion_onehot'], dtype=torch.float32)\n",
    "\n",
    "        audio = audio_preprocessing.read_file(filename)\n",
    "        # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "        # majority. So make all sounds have the same number of channels and same \n",
    "        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "        # result in arrays of different lengths, even though the sound duration is\n",
    "        # the same.\n",
    "        reaud = audio_preprocessing.set_sampling_rate(audio, self.sr)\n",
    "        rechan = audio_preprocessing.set_num_channel(reaud, self.channel)\n",
    "\n",
    "        dur_aud = audio_preprocessing.standardize_audio_length(rechan, self.duration)\n",
    "        shift_aud = audio_preprocessing.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = audio_preprocessing.generate_mfcc_spectrogram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = audio_preprocessing.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "        return aug_sgram, emotion_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "634b4ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset:  15697\n",
      "Size of Training data (%):  70.00063706440721\n",
      "Size of Testing data (%):  19.997451742371155\n",
      "Size of Validation data (%):  10.001911193221636\n"
     ]
    }
   ],
   "source": [
    "# result_df consists of filename and the one-hot encoded emotions\n",
    "dataset = SoundDS(result_df)\n",
    "\n",
    "# Random split with ratios of 70% training, 10% validation, and 20% testing\n",
    "total_items = len(dataset)\n",
    "train_size = round(total_items * 0.7)\n",
    "val_size = round(total_items * 0.1)\n",
    "test_size = total_items - train_size - val_size\n",
    "\n",
    "# Checking dataset split\n",
    "print(\"Size of dataset: \", total_items)\n",
    "print(\"Size of Training data (%): \", train_size / total_items * 100)\n",
    "print(\"Size of Testing data (%): \", test_size / total_items * 100)\n",
    "print(\"Size of Validation data (%): \", val_size / total_items * 100)\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b322156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sample_data:  (batch_sz, num_channels, Mel freq_bands, time_steps) torch.Size([16, 1, 64, 126])\n",
      "Shape of Mel Spectrogram: (num_channels, Mel freq_bands, time_steps in spec) torch.Size([1, 64, 126]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check size of mel spectrogram at the end: \n",
    "# e.g. (num_channels, Mel freq_bands, time_steps in spec) = (2, 64, 344)\n",
    "sample_data, _ = next(iter(train_loader))\n",
    "print(\"Shape of sample_data: \", \"(batch_sz, num_channels, Mel freq_bands, time_steps)\", sample_data.shape)\n",
    "# torch.Size([16, 1, 64, 126])\n",
    "\n",
    "mel_spectrogram = sample_data[0]\n",
    "\n",
    "mel_shape = mel_spectrogram.shape\n",
    "print(\"Shape of Mel Spectrogram:\", \"(num_channels, Mel freq_bands, time_steps in spec)\", mel_shape, \"\\n\")\n",
    "# torch.Size([1, 64, 126])\n",
    "\n",
    "# Spectrogram for 1st channel\n",
    "mel_spectrogram = sample_data[0]\n",
    "\n",
    "mel_spectrogram = mel_spectrogram.squeeze()\n",
    "\n",
    "# Plot the Mel Spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram.numpy(), cmap='inferno', origin='lower')\n",
    "\n",
    "n_freq_masks = 2\n",
    "n_time_masks = 2\n",
    "\n",
    "# Overlay frequency and time masking\n",
    "for _ in range(n_freq_masks):\n",
    "    freq_mask_range = torch.randint(1, mel_spectrogram.size(0)//2, (1,)).item()\n",
    "    freq_mask_start = torch.randint(0, mel_spectrogram.size(0) - freq_mask_range, (1,)).item()\n",
    "    mel_spectrogram[freq_mask_start:freq_mask_start+freq_mask_range, :] = mel_spectrogram.mean()\n",
    "\n",
    "for _ in range(n_time_masks):\n",
    "    time_mask_range = torch.randint(1, mel_spectrogram.size(1)//2, (1,)).item()\n",
    "    time_mask_start = torch.randint(0, mel_spectrogram.size(1) - time_mask_range, (1,)).item()\n",
    "    mel_spectrogram[:, time_mask_start:time_mask_start+time_mask_range] = mel_spectrogram.mean()\n",
    "\n",
    "plt.title('Mel Spectrograms with frequency and time masking augmentation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mel Frequency Bands')\n",
    "plt.colorbar(label='dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7ea017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building Network architecture based on the paper:\n",
    "HumanComputer Interaction with a Real-Time Speech\n",
    "Emotion Recognition with Ensembling Techniques 1D\n",
    "Convolution Neural Network and Attention\n",
    "(https://doi.org/10.3390/s23031386)\n",
    "\n",
    "We are taking the output of CNN as the input of LSTM.\n",
    "CNN captures local patterns in audio features, and\n",
    "LSTM learns temporal dependencies before making final prediction. This supports\n",
    "robust sequence prediction.\n",
    "\n",
    "TO DO/CHECK: Kaimingn initialization??\n",
    "\"\"\"\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_emotions):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        # bn = batch normalization\n",
    "        ####################\n",
    "        # Convolution blocks: conv, batch norm, ReLU, max pooling\n",
    "        # Conv block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(5,5), stride=(1,1), padding=(2,2))\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        # Conv block 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Conv block 3\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3), stride=(1,1), padding =(1,1))\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Conv block 4\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(1,1), padding =(1,1))\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Conv block 5\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=(1,1), padding =(1,1))\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ####################\n",
    "\n",
    "        ####################\n",
    "        # LSTM + attention block\n",
    "        hidden_size=64\n",
    "        self.lstm1 = nn.LSTM(input_size=128, hidden_size=hidden_size, bidirectional=False, batch_first = True)\n",
    "\n",
    "        self.attention_linear = nn.Linear(hidden_size, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, bidirectional=False, batch_first = True)\n",
    "        ####################\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.bn6 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_emotions)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Conv block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Conv block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Conv block 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Conv block 5\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # LSTM + attention block\n",
    "        # output_tensor, hiddenstate = self.lstm()\n",
    "#         print(\"shape of conv output: \", x.shape)\n",
    "        \n",
    "#         conv_emb = x\n",
    "        conv_emb = torch.flatten(x, start_dim=2) # Do not flatten batch dimension and time\n",
    "#         print(\"shape of conv output after flattening: \", conv_emb.shape)\n",
    "        \n",
    "        conv_emb = conv_emb.transpose(1, 2)  # Swap the dimensions\n",
    "        conv_emb = conv_emb.reshape(conv_emb.size(0), conv_emb.size(1), -1)  # Reshape to (batch_size, time_steps, hidden_size)\n",
    "#         print(\"Shape of conv output after reshape: \", conv_emb.shape)\n",
    "        \n",
    "        lstm1_out, (h,c) = self.lstm1(conv_emb) # (batch, time, hidden_size) # expects 128\n",
    "        \n",
    "        # Attention\n",
    "        attention_weights = self.attention_linear(lstm1_out).squeeze(-1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1).unsqueeze(-1)\n",
    "        context_vector = torch.sum(attention_weights * lstm1_out, dim=1)\n",
    "\n",
    "        lstm2_out, _ = self.lstm2(context_vector.unsqueeze(1))\n",
    "        print(\"Shape of lstm2 output: \", lstm2_out.shape)\n",
    "        # Fully connected layers\n",
    "#         fc1_out = self.relu(self.fc1(lstm2_out.squeeze(1)))\n",
    "        fc1_out = self.fc1(lstm2_out.squeeze(1))\n",
    "#         print(\"Shape of fully connected layer 1: \", fc1_out.shape)\n",
    "        fc1_out = self.bn6(fc1_out)\n",
    "        x = self.fc2(fc1_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86f1f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_emotions = 10 # Mel freq_bands\n",
    "net =  CNN_LSTM(num_emotions).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc0dc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(net, input_size=(16, 1, 64, 126))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef7be491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "root_dir = './runs'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "def train_on_features(net, optimizer, device, trainloader, criterion, num_epochs=1):\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    net.train()\n",
    "\n",
    "    loss_min = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        correct_pred = 0\n",
    "        total_pred = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            \n",
    "            # Normalize inputs\n",
    "            mean_inputs, std_inputs = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - mean_inputs) / std_inputs\n",
    "            \n",
    "            # Initializing by zero-ing out gradient\n",
    "            optimizer.zero_grad()\n",
    "#             print(\"shape of input into the model: \", inputs.shape)\n",
    "            # Forward + Backward + Optimization\n",
    "            forward_output = net(inputs) # pred\n",
    "            loss = criterion(forward_output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Prediction\n",
    "            temp, pred = torch.max(forward_output, 1)\n",
    "            label_indices = torch.argmax(labels, dim=1)\n",
    "#             print(\"test: \", temp)\n",
    "#             print(\"Shape of test: \", temp.shape)\n",
    "#             print(\"pred: \", pred)\n",
    "#             print(\"Shape of pred: \", pred.shape)\n",
    "            \n",
    "            # Counting correct predictions\n",
    "            correct_pred += (pred == label_indices).sum().item()\n",
    "            total_pred += pred.shape[0]\n",
    "            \n",
    "            if loss < loss_min:\n",
    "                torch.save(net.state_dict(), os.path.join(root_dir, \"best_ser_model.pth\"))\n",
    "        \n",
    "        num_batches = len(trainloader)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        accuracy = correct_pred / total_pred\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd4143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6154, 0.7679, 0.3961, 0.7578, 0.2855, 0.6109, 0.1189, 0.7700, 0.3415,\n",
      "        0.4490, 0.7992, 0.5356, 1.3754, 0.5923, 0.5847, 0.7530],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 9, 1, 9, 8, 5, 3, 9, 5, 7, 4, 3, 4, 5, 6, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.3003, 0.9142, 0.5589, 0.3653, 0.4947, 0.7640, 0.5247, 0.3828, 0.6366,\n",
      "        0.3887, 0.4512, 0.6425, 0.2557, 0.5793, 0.1438, 1.3130],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 4, 6, 3, 3, 6, 4, 9, 6, 7, 6, 9, 1, 7, 1, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6539, 0.3480, 0.4523, 0.8533, 0.9938, 0.6619, 0.4112, 0.4643, 0.6786,\n",
      "        0.8091, 0.5926, 0.8674, 0.3622, 0.6147, 0.3566, 0.7420],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 6, 6, 4, 4, 9, 5, 1, 5, 6, 3, 8, 9, 6, 7, 7], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.4665, 0.2557, 0.6944, 0.5727, 0.4023, 1.1536, 0.3850, 0.5767, 0.6475,\n",
      "        0.4070, 0.5336, 0.6113, 0.5545, 0.7634, 0.5815, 0.6683],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([4, 9, 6, 7, 3, 4, 7, 6, 5, 9, 6, 6, 4, 5, 6, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.9200, 0.3599, 1.2936, 1.1520, 0.4095, 0.1188, 0.2456, 0.7626, 0.5888,\n",
      "        0.2677, 0.5426, 0.3090, 0.6937, 0.7893, 0.9783, 0.8288],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 3, 9, 6, 7, 5, 0, 4, 5, 5, 6, 5, 4, 5, 9, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.5223, 0.5456, 0.5262, 0.7615, 0.5742, 0.5201, 0.2565, 0.2686, 1.2949,\n",
      "        0.4694, 0.9251, 0.6577, 0.5988, 0.6207, 1.1959, 0.3758],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([9, 9, 4, 6, 7, 7, 7, 1, 4, 4, 9, 5, 5, 1, 6, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.9121, 0.5078, 0.4954, 0.5197, 0.6231, 0.6384, 0.4540, 0.3896, 1.5851,\n",
      "        0.3963, 0.2964, 1.0200, 0.5664, 0.3443, 0.4473, 0.4168],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([7, 9, 3, 8, 0, 6, 9, 5, 6, 6, 6, 5, 6, 5, 0, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6251, 0.2755, 0.8030, 0.7615, 0.7606, 0.5131, 0.3170, 0.9888, 0.3922,\n",
      "        0.4665, 0.8229, 0.7770, 0.5254, 1.0067, 0.6224, 0.3460],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([9, 5, 7, 4, 0, 3, 1, 5, 6, 9, 9, 0, 6, 6, 6, 7], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6877, 0.9627, 0.5426, 0.4209, 0.6631, 0.5324, 0.3566, 0.7276, 0.7628,\n",
      "        0.4684, 0.7133, 0.4827, 0.7507, 0.8090, 0.4668, 0.9442],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([9, 8, 6, 5, 5, 9, 1, 6, 9, 5, 9, 7, 5, 8, 0, 7], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.8009, 0.3793, 0.6661, 0.4849, 0.6016, 0.8805, 0.0955, 1.2864, 0.3126,\n",
      "        0.9042, 0.7408, 0.5113, 0.7179, 0.7812, 0.9132, 0.9796],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 4, 5, 8, 5, 5, 3, 8, 4, 6, 9, 3, 7, 9, 8, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.2397, 0.7169, 0.9470, 0.8115, 0.3195, 0.7980, 0.9347, 0.4125, 0.5029,\n",
      "        1.0776, 0.7636, 0.3224, 0.3620, 0.5792, 1.2062, 0.3963],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 7, 6, 6, 4, 0, 9, 6, 7, 9, 9, 9, 9, 3, 0, 7], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.4735, 0.7394, 0.4568, 1.1118, 0.7576, 0.6571, 0.4311, 0.5517, 0.2946,\n",
      "        0.3609, 0.9178, 0.3064, 0.4304, 0.6634, 0.2502, 1.0687],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([4, 6, 0, 9, 6, 4, 5, 6, 6, 5, 0, 5, 5, 4, 7, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.5584, 0.8276, 0.2995, 1.1970, 0.2804, 0.5861, 0.6955, 0.6664, 0.4958,\n",
      "        0.3408, 1.0846, 0.2960, 0.3993, 1.0307, 0.8434, 0.5922],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 9, 4, 6, 7, 6, 0, 6, 9, 0, 6, 5, 5, 0, 6, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6557, 0.5468, 0.5273, 0.8081, 0.4627, 0.7809, 0.9412, 1.2616, 1.3664,\n",
      "        0.4328, 0.8079, 0.6606, 0.2744, 0.5351, 0.2644, 0.7164],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([7, 6, 4, 6, 8, 5, 9, 8, 9, 0, 8, 5, 9, 9, 9, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.0353, 0.7522, 0.3389, 0.8413, 0.3890, 0.6639, 0.7858, 0.3791, 0.6306,\n",
      "        1.0553, 0.6605, 0.6504, 0.5439, 0.7295, 0.6125, 0.9081],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 5, 5, 9, 6, 5, 8, 0, 9, 6, 4, 9, 6, 8, 3, 0], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6015, 0.0741, 0.4455, 0.5335, 0.5878, 0.9349, 0.8104, 0.5538, 0.7288,\n",
      "        0.4092, 0.8291, 0.9139, 0.8505, 1.1886, 0.3334, 1.2041],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([9, 5, 6, 4, 4, 6, 6, 6, 6, 0, 8, 0, 0, 6, 5, 8], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.8062, 0.6616, 0.8682, 1.2497, 0.3996, 0.6924, 0.5793, 0.7968, 0.5291,\n",
      "        0.5460, 0.5248, 0.5003, 0.8805, 0.5196, 0.6756, 0.5955],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 0, 4, 8, 4, 0, 6, 9, 6, 8, 5, 5, 6, 9, 6, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6020, 1.4201, 0.4135, 0.6965, 0.4051, 1.1051, 0.7001, 0.6243, 0.2949,\n",
      "        1.2736, 1.1598, 0.8135, 0.5011, 0.9399, 0.9011, 0.3740],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 8, 0, 6, 0, 8, 6, 0, 7, 4, 6, 9, 6, 6, 9, 1], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.5455, 1.2336, 0.3372, 0.1917, 0.8373, 0.3531, 0.5413, 0.4139, 1.1157,\n",
      "        0.2647, 0.7845, 1.0239, 0.3224, 0.5324, 0.9580, 0.7751],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 8, 6, 9, 9, 0, 9, 0, 8, 9, 6, 8, 1, 6, 5, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.7841, 0.6897, 0.4437, 0.7419, 0.6321, 1.0329, 0.3724, 0.6140, 0.5446,\n",
      "        1.5027, 0.4441, 0.8611, 0.6920, 0.8471, 0.2059, 0.3590],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([9, 6, 3, 9, 6, 8, 5, 9, 7, 8, 0, 6, 8, 7, 5, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.7866, 0.3498, 0.2053, 0.7007, 0.7763, 1.4458, 0.6592, 1.3055, 0.6577,\n",
      "        0.8118, 0.5219, 0.7019, 0.5081, 0.5040, 0.7845, 0.5699],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 5, 6, 6, 9, 8, 6, 9, 0, 6, 4, 7, 5, 7, 0, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.7998, 0.8796, 0.3772, 0.3462, 0.9397, 0.4762, 0.8702, 0.7829, 0.6770,\n",
      "        0.8085, 0.8475, 0.7143, 0.9563, 0.6597, 0.9729, 0.5511],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([4, 8, 9, 5, 8, 6, 7, 9, 6, 9, 6, 0, 9, 6, 4, 5], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.2558, 0.0540, 0.6926, 0.8655, 0.3893, 1.2850, 0.7244, 0.2971, 0.5455,\n",
      "        0.6541, 0.6585, 0.7512, 0.8197, 0.9015, 0.5739, 0.7812],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 7, 9, 9, 6, 6, 8, 5, 6, 9, 7, 6, 6, 8, 0, 5], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.5651, 0.1031, 0.5754, 0.6082, 1.4591, 1.1289, 0.7233, 1.0105, 0.1721,\n",
      "        0.4254, 0.6722, 1.2800, 0.8389, 0.3282, 0.7813, 0.6222],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 1, 9, 6, 8, 9, 6, 6, 7, 0, 5, 8, 8, 7, 0, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.0029, 0.6533, 0.4971, 0.1167, 0.8464, 0.7601, 0.8720, 0.6409, 0.0679,\n",
      "        0.6339, 0.6054, 1.0745, 1.0781, 0.8663, 1.0829, 0.7774],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([7, 6, 9, 5, 7, 6, 9, 5, 6, 5, 6, 8, 8, 7, 4, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.5978, 1.0098, 0.2433, 0.7872, 0.4819, 0.8619, 0.4149, 0.8908, 1.1289,\n",
      "        0.6335, 0.4449, 0.6732, 0.7307, 0.6721, 0.5987, 1.1783],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([9, 7, 6, 7, 8, 4, 6, 6, 6, 6, 5, 6, 0, 5, 0, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.0072, 0.5878, 0.7129, 0.5694, 0.3116, 0.3568, 0.7682, 0.3452, 0.7153,\n",
      "        0.8323, 0.8121, 0.6114, 1.6462, 0.3037, 1.2976, 0.0858],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 4, 9, 6, 9, 9, 6, 5, 5, 6, 8, 8, 8, 7, 7, 1], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.5252, 0.4467, 0.7013, 0.4546, 0.9967, 0.8947, 0.5997, 0.5723, 1.2128,\n",
      "        0.6624, 1.0708, 0.5115, 0.4598, 1.3551, 0.8923, 0.9519],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 8, 5, 3, 4, 7, 6, 9, 7, 6, 8, 6, 5, 8, 9, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.4182, 0.5840, 1.0784, 0.9575, 0.5032, 1.0409, 0.5477, 0.7670, 0.1015,\n",
      "        0.9305, 0.5343, 0.6983, 0.6314, 0.9370, 1.2457, 0.6998],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([7, 9, 6, 4, 5, 8, 6, 9, 6, 8, 6, 9, 5, 0, 7, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.7292, 0.5616, 0.4250, 0.7557, 0.6116, 0.5414, 0.5076, 1.2242, 0.4303,\n",
      "        0.4794, 0.9614, 0.7194, 1.0488, 1.1342, 1.3681, 0.6133],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 4, 8, 8, 5, 9, 7, 6, 7, 6, 0, 6, 9, 8, 7, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.7712, 0.6145, 1.2852, 0.9820, 0.5483, 0.3048, 1.2145, 0.9785, 0.7685,\n",
      "        1.1477, 0.3126, 0.7605, 0.6081, 0.4795, 1.2491, 0.5480],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([0, 5, 7, 9, 7, 6, 6, 6, 6, 4, 9, 4, 5, 6, 7, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.9871, 0.6112, 0.8083, 0.8621, 1.5493, 0.9023, 1.2132, 0.4513, 1.1819,\n",
      "        0.6402, 0.4758, 1.1948, 0.9403, 0.2815, 0.9262, 0.8570],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 9, 6, 9, 7, 6, 8, 0, 4, 9, 7, 8, 4, 7, 7, 6], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.0483, 0.5844, 1.2772, 0.4470, 0.5346, 1.3088, 1.4293, 0.6032, 0.7323,\n",
      "        0.7995, 0.4637, 0.5463, 0.6532, 0.7889, 1.1577, 0.7955],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 6, 9, 9, 4, 6, 7, 5, 5, 9, 6, 8, 9, 6, 7, 8], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.8594, 0.6195, 0.6452, 0.6998, 1.6788, 0.8962, 1.4424, 0.7300, 0.5149,\n",
      "        0.6617, 0.6887, 1.3117, 1.2361, 1.0524, 1.0445, 0.5357],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([9, 8, 7, 9, 8, 6, 4, 5, 7, 6, 6, 0, 7, 6, 6, 7], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.0967, 1.1574, 0.8864, 0.4785, 1.3087, 0.9806, 1.1071, 0.5833, 0.9921,\n",
      "        0.7659, 1.8932, 0.9618, 0.8996, 0.1671, 0.9452, 0.4957],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([4, 7, 5, 0, 9, 9, 4, 9, 6, 4, 8, 6, 9, 0, 8, 0], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.7169, 0.5921, 0.5175, 0.4009, 1.4448, 0.8086, 0.9480, 0.6507, 1.1034,\n",
      "        0.9332, 1.0502, 0.5759, 0.4625, 1.0189, 0.7831, 0.6343],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 6, 5, 0, 7, 9, 9, 6, 8, 5, 8, 9, 0, 4, 0, 4], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.5781, 1.1712, 0.9604, 1.4557, 0.9143, 1.2864, 0.8562, 1.1126, 0.3983,\n",
      "        0.5008, 0.8854, 0.7286, 0.4648, 0.6066, 0.6997, 0.5076],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 8, 6, 7, 4, 6, 6, 8, 9, 6, 9, 9, 9, 7, 0, 0], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.8141, 0.7366, 0.8415, 0.6526, 0.8148, 0.7437, 0.9109, 0.4435, 1.0315,\n",
      "        1.2324, 1.5754, 0.8137, 1.2183, 0.9611, 0.6488, 0.5288],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 0, 5, 5, 7, 9, 7, 9, 7, 7, 8, 8, 6, 9, 6, 5], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.8577, 1.1705, 0.8677, 1.1535, 0.5978, 0.7027, 0.3556, 1.7649, 0.4177,\n",
      "        0.9662, 0.5486, 0.7001, 1.0279, 0.8712, 2.3337, 0.9301],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 8, 6, 7, 6, 9, 9, 8, 9, 6, 5, 9, 6, 9, 4, 0], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([ 0.9305,  0.2191,  0.7324,  1.2265,  0.5975,  1.5473,  0.4561,  2.0449,\n",
      "         0.3828,  1.2353,  0.9327, -0.0294,  0.5409,  0.8057,  0.5763,  0.8697],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 7, 6, 8, 0, 9, 8, 8, 9, 7, 9, 7, 9, 0, 3, 5], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.4256, 1.4456, 0.3318, 0.7920, 0.6478, 1.5936, 0.9841, 0.6335, 0.6144,\n",
      "        0.7162, 0.7500, 1.0538, 1.8473, 0.5434, 1.1310, 0.8461],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 4, 9, 9, 0, 7, 6, 9, 5, 6, 6, 9, 8, 9, 8, 8], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.3048, 0.7105, 0.7428, 1.0108, 0.7258, 0.9986, 1.3007, 0.7030, 0.8148,\n",
      "        0.6312, 1.8287, 0.6591, 1.6204, 0.4712, 0.6893, 0.6079],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 9, 0, 7, 6, 0, 4, 6, 6, 5, 4, 6, 8, 9, 9, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6082, 0.9411, 1.1136, 0.6015, 0.8983, 0.8189, 0.6474, 1.1315, 1.1101,\n",
      "        0.7421, 0.6540, 0.7558, 1.2053, 0.6086, 0.5386, 1.8067],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 4, 6, 9, 6, 0, 5, 4, 7, 8, 6, 9, 0, 6, 5, 8], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.2203, 0.3638, 0.6227, 1.1189, 1.5199, 1.3298, 0.4832, 1.1708, 0.6993,\n",
      "        0.8194, 0.6142, 0.9458, 0.6354, 1.1541, 0.6749, 1.1406],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 5, 6, 8, 4, 0, 9, 6, 9, 9, 0, 5, 7, 8, 6, 0], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6717, 0.6955, 0.4791, 0.7264, 0.3700, 0.7883, 1.2882, 1.2279, 0.7664,\n",
      "        0.7120, 1.2496, 0.3987, 0.7720, 1.1396, 1.1753, 0.9711],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([6, 9, 9, 9, 4, 9, 8, 6, 0, 6, 0, 6, 4, 8, 7, 8], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.6477, 0.6529, 0.2956, 1.3547, 0.9789, 0.5719, 0.3917, 1.1451, 0.2869,\n",
      "        0.7151, 0.8679, 0.8161, 1.0936, 0.6930, 0.7050, 1.3887],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([7, 7, 1, 4, 4, 7, 7, 6, 6, 5, 9, 8, 4, 6, 0, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([0.3463, 1.1167, 0.6591, 1.2866, 0.2825, 0.4850, 1.1395, 0.8343, 1.0084,\n",
      "        1.6500, 1.1015, 0.9070, 1.3559, 1.3740, 0.8548, 0.8141],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([5, 9, 0, 8, 5, 6, 8, 7, 9, 8, 9, 4, 6, 7, 0, 7], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n",
      "shape of input into the model:  torch.Size([16, 1, 64, 126])\n",
      "shape of conv output:  torch.Size([16, 128, 2, 3])\n",
      "shape of conv output after flattening:  torch.Size([16, 128, 6])\n",
      "Shape of conv output after reshape:  torch.Size([16, 6, 128])\n",
      "Shape of lstm2 output:  torch.Size([16, 1, 64])\n",
      "Shape of fully connected layer 1:  torch.Size([16, 64])\n",
      "test:  tensor([1.1720, 1.6860, 0.8510, 1.5403, 1.3229, 1.1019, 0.8433, 0.7425, 0.8015,\n",
      "        0.6928, 0.6277, 0.4435, 0.7325, 0.9499, 1.2787, 0.7503],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "Shape of test:  torch.Size([16])\n",
      "pred:  tensor([8, 8, 5, 4, 7, 9, 0, 5, 6, 1, 0, 6, 0, 6, 4, 9], device='cuda:0')\n",
      "Shape of pred:  torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "Net = train_on_features(net, optimizer, device, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecceaf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped input: torch.Size([16, 64, 126])\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "# Assuming your input data has the shape (batch_sz, num_channels, Mel freq_bands, time_steps)\n",
    "# input_data = torch.randn(16, 1, 64, 126)  # Example input data\n",
    "\n",
    "# # Reshape the input data to flatten along the time axis (time_steps)\n",
    "# # The new shape will be (batch_sz, Mel freq_bands, time_steps)\n",
    "# reshaped_input = input_data.permute(0, 2, 3, 1).reshape(input_data.size(0), input_data.size(2), -1)\n",
    "\n",
    "# # Check the shape of the reshaped input\n",
    "# print(\"Shape of reshaped input:\", reshaped_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48199cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Feature Extraction\n",
    "# 1) Wav2vec2 model\n",
    "# '''\n",
    "# bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H # can choose another wav2vec2 model\n",
    "# print(\"Sample rate: \", bundle.sample_rate)\n",
    "# print(\"Labels: \", bundle.get_labels())\n",
    "# model = bundle.get_model().to(device)\n",
    "\n",
    "# # Example audio\n",
    "# SPEECH_FILE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
    "# IPython.display.Audio(SPEECH_FILE)\n",
    "# waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "# waveform = waveform.to(device)\n",
    "# # Resample example audio if its sample rate doesn't match the pipeline's sample rate\n",
    "# if sample_rate != bundle.sample_rate:\n",
    "#     print(\"Audio vs Bundle sample rate: \", sample_rate, bundle.sample_rate)\n",
    "#     waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    \n",
    "# # Extract features\n",
    "# with torch.inference_mode():\n",
    "#     features, _ = model.extract_features(waveform)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d89a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Feature Extraction\n",
    "# 2) Hand-crafted features: ZCR, RMSE, MFCC\n",
    "# '''\n",
    "# X, sample_rate = librosa.load('./datasets/berlin-database-of-emotional-speech-emodb/wav/03a01Fa.wav')\n",
    "# mfccs = np.mean(librosa.feature.mfcc(y=X, n_mfcc=25,), axis = 0) # calculating mean?\n",
    "# rms = librosa.feature.rms(y=X) # root mean square value for each frame of audio sample\n",
    "# zcr = librosa.feature.zero_crossing_rate(y=X) # zero crossing rate of audio time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f85300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's visualize what we are doing!\n",
    "# # some of this code taken from https://github.com/MiteshPuthran/Speech-Emotion-Analyzer/blob/master/final_results_gender_test.ipynb\n",
    "# import librosa.display\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# librosa.display.waveshow(X, sr=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae3d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting features into dataframes\n",
    "# needs \"/\" at end of filepath\n",
    "# data_filepath = \"./datasets/berlin-database-of-emotional-speech-emodb/wav/\"\n",
    "\n",
    "# database_name = \"berlin-database-of-emotional-speech-emodb\"\n",
    "# filenames_list = os.listdir(data_filepath) # filenames without full filepath\n",
    "# full_filenames_list = [data_filepath + filename for filename in filenames_list] # adding full filepath\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17112979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract features of all audio files into a dataframe\n",
    "# import pandas as pd\n",
    "\n",
    "# features_df = pd.DataFrame(columns=['filename', 'mfccs', 'rms', 'zcr', 'emotion'])\n",
    "\n",
    "# # works for a single folder of audio files at once, all from the same dataset\n",
    "# for i, filename in enumerate(full_filenames_list):\n",
    "#     X, sample_rate = librosa.load(filename) # load waveform\n",
    "#     mfccs = np.mean(librosa.feature.mfcc(y=X, n_mfcc=25,), axis = 0) # calculate feature values\n",
    "#     rms = librosa.feature.rms(y=X)\n",
    "#     zcr = librosa.feature.zero_crossing_rate(y=X)\n",
    "#     features_df.at[i, 'mfccs'] = mfccs # save X features to dataframe\n",
    "#     features_df.at[i, 'rms'] = np.array(rms)\n",
    "#     features_df.at[i, 'zcr'] = np.array(zcr)\n",
    "    \n",
    "#     row_index = df.loc[df['filename'] == filename].index[0] # finding the correct emotion for the filename\n",
    "#     features_df.at[i, 'emotion'] = df.at[row_index,'emotion'] # selects correct emotion from Noah's df\n",
    "    \n",
    "#     split_name = filename.split('/') # get the correct filename\n",
    "#     features_df.at[i, 'filename'] = split_name[-1]\n",
    "    \n",
    "#     #features_df.at[i,'emotion'] = df.loc['im','emotion']\n",
    "\n",
    "# #features_df = pd.concat([features_df, labels_df], axis=1)\n",
    "# print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aadf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (428, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's shuffle these boiz\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(features_df, test_size=0.2, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ea1b57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m net\n\u001b[1;32m     30\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m DataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 14\u001b[0m, in \u001b[0;36mtrain_on_features\u001b[0;34m(net, optimizer, device, trainloader, critrerion, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     13\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m---> 14\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m     pred \u001b[38;5;241m=\u001b[39m net(inputs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# # Training loop\n",
    "# root_dir = './runs'\n",
    "# os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "# def train_on_features(net, optimizer, device, trainloader, critrerion, epochs=1):\n",
    "#     if torch.cuda.is_available():\n",
    "#         net.cuda()\n",
    "#     net.train()\n",
    "\n",
    "#     loss_min = float('inf')\n",
    "#     for epoch in range(epochs):\n",
    "#         for i, data in enumerate(trainloader):\n",
    "#             inputs, labels = data\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             pred = net(inputs)\n",
    "#             loss = criterion(pred, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             if loss < loss_min:\n",
    "#                 torch.save(net.state_dict(), os.path.join(root_dir, \"best_ser_model.pth\"))\n",
    "\n",
    "\n",
    "#     print('Finished Training')\n",
    "#     return net\n",
    "\n",
    "# net = train_on_features(net, optimizer, device, trainloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "724f9822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 9.1553e-05, -3.0518e-04, -7.9346e-04,  ..., -1.2207e-03,\n",
      "        -1.4343e-03, -1.5259e-03]), 'anxiety')\n"
     ]
    }
   ],
   "source": [
    "#net = train_on_features(net, optimizer, device, trainloader, criterion)\n",
    "# trainloader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=1)\n",
    "# for i, data in enumerate(trainloader):\n",
    "#     print(data)\n",
    "# print(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d64990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
