{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EWmBPG6JWDr",
        "outputId": "e6c7119c-eb32-482d-c042-6bfbe1b70925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# check GPU status\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After extracting features, we are doing classification with 1DCNN + LSTM with attention model.\n",
        "Features: ZCR, RMSE, and MFCC"
      ],
      "metadata": {
        "id": "x2AOI2GKAyQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets"
      ],
      "metadata": {
        "id": "yMTIDuyfLsXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "_GQR8GmJCdmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_of_data ="
      ],
      "metadata": {
        "id": "9bmlXjWkLwF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# Before feeding extracted audio features into the model, we need to make the\n",
        "# features look like an image. This is because CNN expects an image as input.\n",
        "\n",
        "# The following code pads audio features to ensure that each channels are of the\n",
        "# same size.\n",
        "# \"\"\"\n",
        "# # This code was adapted from Nicolas Gervais on https://stackoverflow.com/questions/59241216/padding-numpy-arrays-to-a-specific-size on 1/10/2021\n",
        "# def padding(array, xx, yy):\n",
        "#     \"\"\"\n",
        "#     :param array: numpy array\n",
        "#     :param xx: desired height\n",
        "#     :param yy: desirex width\n",
        "#     :return: padded array\n",
        "#     \"\"\"\n",
        "#     h = array.shape[0]\n",
        "#     w = array.shape[1]\n",
        "\n",
        "#     a = max((xx - h) // 2,0)\n",
        "#     aa = max(0,xx - a - h)\n",
        "\n",
        "#     b = max(0,(yy - w) // 2)\n",
        "#     bb = max(yy - b - w,0)\n",
        "\n",
        "#     return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')"
      ],
      "metadata": {
        "id": "wVyBplsIOd2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN with LSTM architecture is based on:\n",
        "\n",
        "Figure 4. from Human–Computer Interaction with a Real-Time Speech\n",
        "Emotion Recognition with Ensembling Techniques 1D\n",
        "Convolution Neural Network and Attention\n",
        "(https://doi.org/10.3390/s23031386)\n"
      ],
      "metadata": {
        "id": "9MiZxBW6_8Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Building Network architecture based on the paper:\n",
        "Human–Computer Interaction with a Real-Time Speech\n",
        "Emotion Recognition with Ensembling Techniques 1D\n",
        "Convolution Neural Network and Attention\n",
        "(https://doi.org/10.3390/s23031386)\n",
        "\n",
        "We are taking the output of CNN as the input of LSTM.\n",
        "CNN captures local patterns in audio features, and\n",
        "LSTM learns temporal dependencies before making final prediction. This supports\n",
        "robust sequence prediction.\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        # bn = batch normalization\n",
        "        ####################\n",
        "        # Convolution blocks: 1dconv, batch norm, ReLU, max pooling\n",
        "        # Conv block 1\n",
        "        self.conv1 = nn.Conv1d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1)\n",
        "        self.bn1 = nn.BatchNorm1d(8)\n",
        "\n",
        "        # Conv block 2\n",
        "        self.conv2 = nn.Conv1d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "\n",
        "        # Conv block 3\n",
        "        self.conv3 = nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # Conv block 4\n",
        "        self.conv4 = nn.Conv1d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "\n",
        "        # Conv block 5\n",
        "        self.conv5 = nn.Conv1d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
        "        ####################\n",
        "\n",
        "        ####################\n",
        "        # LSTM + attention block\n",
        "        self.lstm1 = nn.LSTM(input_size = 128, hidden_size = 64, num_layers = 1, bias = True)\n",
        "\n",
        "        self.attention = nn.Linear(64, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(input_size = 64, hidden_size = 64, num_layers = 1, bias = True)\n",
        "        ####################\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(64, 32) # May need to increase 32 to capture more complex data (?)\n",
        "        self.bn6 = nn.BatchNorm1d(32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 4\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 5\n",
        "        x = self.conv5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # LSTM + attention block\n",
        "        # output_tensor, hiddenstate = self.lstm()\n",
        "        lstm1_out, _ = self.lstm1(x)\n",
        "\n",
        "        attention_weights = self.softmax(self.attention(lstm1_out))\n",
        "        context = torch.sum(attention_weights * lstm1_out, dim=1)\n",
        "\n",
        "        lstm2_out, _ = self.lstm2(context.unsqueeze(-1))\n",
        "\n",
        "        # fully connected layers and softmax\n",
        "        x = self.fc1(lstm2_out.squeeze(1))\n",
        "        x = self.bn6(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Softmax for prediction\n",
        "        # F.softmax is used instead of self.softmax because it is not associated\n",
        "        # with any parameters\n",
        "        x = F.softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "mB2sDnMnd_9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "net =  CNN_LSTM().to(device)\n",
        "optimizer =  optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "u5pT-r4W4zYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SvOAQzdq-UFe"
      }
    }
  ]
}