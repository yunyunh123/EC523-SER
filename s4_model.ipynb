{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00ff9b56646f4fc",
   "metadata": {},
   "source": [
    "This notebook is for S4 architecture building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff8dd447de5ae3",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d487b376a3022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /usr4/dl523/nbmarkow/.local/lib/python3.10/site-packages (0.7.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b17bbe13f1d022e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:17:17.419180Z",
     "start_time": "2024-04-20T18:17:17.414711Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "\n",
    "# Settings\n",
    "# Presets from S4 example\n",
    "n_layers=4\n",
    "d_model=126 # Model dimension\n",
    "dropout=0.1\n",
    "prenorm=True\n",
    "batch_size=4\n",
    "num_workers=4\n",
    "\n",
    "spec_shape = (201,203)\n",
    "\n",
    "d_input = 126 #spec_shape[1] # Because input is grayscale image. Make 3 for RGB\n",
    "d_output = 10\n",
    "\n",
    "# Use to load from a checkpoint\n",
    "start_epoch = 0\n",
    "\n",
    "fs = 16000\n",
    "\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-2\n",
    "epochs=100\n",
    "\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Specify the directory you want the datasets to be contained in\n",
    "pdir=\"/projectnb/dl523/students/nbmarkow/term_project/EC523-SER\"\n",
    "dataset_dir = op.join(pdir,\"datasets\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(op.join(pdir,\"s4\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cb870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47d6835928f677fb",
   "metadata": {},
   "source": [
    "# Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960ae1941ac65cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:17:20.843095Z",
     "start_time": "2024-04-20T18:17:19.875530Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset import download_datasets, SpeechEmotionDataset, get_dataset_info, audio_preprocessing, SoundDS\n",
    "\n",
    "import torchaudio.transforms as t\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "# Download a single dataset\n",
    "#download_datasets(dataset_dir)\n",
    "\n",
    "# Download the rest of the datasets available\n",
    "#download_datasets(dataset_dir)\n",
    "\n",
    "# Acquire info on datasets (those that have functions to get data for)\n",
    "df = get_dataset_info(dataset_dir)\n",
    "\n",
    "# Make into a Dataset object that a pytorch optimizer can use\n",
    "# Can optionally specify a sampling rate for all audio files to be in\n",
    "transforms = t.Spectrogram(n_fft=400)\n",
    "trainset = SpeechEmotionDataset(df, fs=16000, transform=transforms)\n",
    "\n",
    "\n",
    "transforms = Lambda(lambda x: x.view(1, 8064))\n",
    "trainset = SoundDS(df, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6047ecf19136748b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:17:22.702200Z",
     "start_time": "2024-04-20T18:17:22.692891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8064])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check it works\n",
    "dataiter = iter(trainset)\n",
    "data, label = next(dataiter)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eab8c156d4277e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T17:37:17.622904Z",
     "start_time": "2024-04-20T17:37:17.335442Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/6778756.1.ece/ipykernel_315065/1127621095.py:2: UserWarning: Only one segment is calculated since parameter NFFT (=256) >= signal length (=1).\n",
      "  plt.specgram(data.numpy())\n",
      "/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/matplotlib/axes/_axes.py:7774: RuntimeWarning: divide by zero encountered in log10\n",
      "  Z = 10. * np.log10(spec)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.specgram(data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac37821138265e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T17:37:19.886272Z",
     "start_time": "2024-04-20T17:37:19.881050Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af311d93f47dbfd",
   "metadata": {},
   "source": [
    "# Get dataset as pandas dataframe and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d73d9518ef30e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speaker_n</th>\n",
       "      <th>intensity</th>\n",
       "      <th>emotion</th>\n",
       "      <th>version</th>\n",
       "      <th>language</th>\n",
       "      <th>database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>16</td>\n",
       "      <td>NA</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>a</td>\n",
       "      <td>german</td>\n",
       "      <td>emodb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>10</td>\n",
       "      <td>NA</td>\n",
       "      <td>bored</td>\n",
       "      <td>b</td>\n",
       "      <td>german</td>\n",
       "      <td>emodb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>16</td>\n",
       "      <td>NA</td>\n",
       "      <td>bored</td>\n",
       "      <td>a</td>\n",
       "      <td>german</td>\n",
       "      <td>emodb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>16</td>\n",
       "      <td>NA</td>\n",
       "      <td>neutral</td>\n",
       "      <td>b</td>\n",
       "      <td>german</td>\n",
       "      <td>emodb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>14</td>\n",
       "      <td>NA</td>\n",
       "      <td>bored</td>\n",
       "      <td>a</td>\n",
       "      <td>german</td>\n",
       "      <td>emodb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12692</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>3</td>\n",
       "      <td>NA</td>\n",
       "      <td>surprise</td>\n",
       "      <td>05</td>\n",
       "      <td>english</td>\n",
       "      <td>savee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12693</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>3</td>\n",
       "      <td>NA</td>\n",
       "      <td>neutral</td>\n",
       "      <td>11</td>\n",
       "      <td>english</td>\n",
       "      <td>savee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12694</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>3</td>\n",
       "      <td>NA</td>\n",
       "      <td>neutral</td>\n",
       "      <td>14</td>\n",
       "      <td>english</td>\n",
       "      <td>savee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12695</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>3</td>\n",
       "      <td>NA</td>\n",
       "      <td>neutral</td>\n",
       "      <td>08</td>\n",
       "      <td>english</td>\n",
       "      <td>savee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12696</th>\n",
       "      <td>/projectnb/dl523/students/nbmarkow/term_projec...</td>\n",
       "      <td>3</td>\n",
       "      <td>NA</td>\n",
       "      <td>sadness</td>\n",
       "      <td>14</td>\n",
       "      <td>english</td>\n",
       "      <td>savee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12697 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename speaker_n intensity  \\\n",
       "0      /projectnb/dl523/students/nbmarkow/term_projec...        16        NA   \n",
       "1      /projectnb/dl523/students/nbmarkow/term_projec...        10        NA   \n",
       "2      /projectnb/dl523/students/nbmarkow/term_projec...        16        NA   \n",
       "3      /projectnb/dl523/students/nbmarkow/term_projec...        16        NA   \n",
       "4      /projectnb/dl523/students/nbmarkow/term_projec...        14        NA   \n",
       "...                                                  ...       ...       ...   \n",
       "12692  /projectnb/dl523/students/nbmarkow/term_projec...         3        NA   \n",
       "12693  /projectnb/dl523/students/nbmarkow/term_projec...         3        NA   \n",
       "12694  /projectnb/dl523/students/nbmarkow/term_projec...         3        NA   \n",
       "12695  /projectnb/dl523/students/nbmarkow/term_projec...         3        NA   \n",
       "12696  /projectnb/dl523/students/nbmarkow/term_projec...         3        NA   \n",
       "\n",
       "        emotion version language database  \n",
       "0       anxiety       a   german    emodb  \n",
       "1         bored       b   german    emodb  \n",
       "2         bored       a   german    emodb  \n",
       "3       neutral       b   german    emodb  \n",
       "4         bored       a   german    emodb  \n",
       "...         ...     ...      ...      ...  \n",
       "12692  surprise      05  english    savee  \n",
       "12693   neutral      11  english    savee  \n",
       "12694   neutral      14  english    savee  \n",
       "12695   neutral      08  english    savee  \n",
       "12696   sadness      14  english    savee  \n",
       "\n",
       "[12697 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acquire info on datasets (those that have functions to get data for)\n",
    "df = get_dataset_info(dataset_dir)\n",
    "#df = df[ df[\"language\"]!=\"persian\",:]\n",
    "df = df.drop(df[df.language==\"persian\"].index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747d3971f3cde41",
   "metadata": {},
   "source": [
    "# Create DataLoaders for train, test, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ebb02e74f9a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train, val_split):\n",
    "    train_len = int(len(train) * (1.0-val_split))\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        train,\n",
    "        (train_len, len(train) - train_len),\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return train, val\n",
    "\n",
    "\n",
    "# Specify transforms to use\n",
    "transforms = t.Spectrogram(n_fft=400)\n",
    "\n",
    "# Make trainset\n",
    "#trainset = SpeechEmotionDataset(df, fs=fs, transform=transforms)\n",
    "trainset = SoundDS(df)\n",
    "trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Make valset\n",
    "#valset = SpeechEmotionDataset(df, fs=fs, transform=transforms)\n",
    "valset = SoundDS(df)\n",
    "_, valset = split_train_val(valset, val_split=0.1)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Make testset\n",
    "testset = SoundDS(df)\n",
    "#testset = SpeechEmotionDataset(df, fs=fs, transform=transforms)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d6724",
   "metadata": {},
   "source": [
    "# Dissect S4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d18024a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 126])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x126 and 201x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 70\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mInput x is shape (B, L, d_input)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, L, d_input) -> (B, L, d_model)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, norm, dropout \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(s4_layers, norms, dropouts):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x126 and 201x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from s4.models.s4.s4d import S4D\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Settings\n",
    "spec_shape = (201,203)\n",
    "\n",
    "n_layers=4\n",
    "d_model=256 # Model dimension\n",
    "dropout=0.1\n",
    "prenorm=True\n",
    "batch_size=64\n",
    "num_workers=4\n",
    "d_input = spec_shape[0] # Because input is grayscale image. Make 3 for RGB\n",
    "d_output = 10\n",
    "fs = 16000\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-2\n",
    "prenorm=True\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Get data\n",
    "dataiter = iter(trainset)\n",
    "x, label = next(dataiter)\n",
    "\n",
    "print(x.shape)\n",
    "#--------------------------------------------------------\n",
    "# Dropout fn\n",
    "\n",
    "# Dropout broke in PyTorch 1.11\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) == (1, 11):\n",
    "    print(\"WARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\")\n",
    "    dropout_fn = nn.Dropout\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 12):\n",
    "    dropout_fn = nn.Dropout1d\n",
    "else:\n",
    "    dropout_fn = nn.Dropout2d\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Construct model\n",
    "\n",
    "# Linear encoder (d_input = 1 for grayscale and 3 for RGB)\n",
    "encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "# # Stack S4 layers as residual blocks\n",
    "s4_layers = nn.ModuleList()\n",
    "norms = nn.ModuleList()\n",
    "dropouts = nn.ModuleList()\n",
    "for _ in range(n_layers):\n",
    "    s4_layers.append(\n",
    "        S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, lr))\n",
    "    )\n",
    "    norms.append(nn.LayerNorm(d_model))\n",
    "    dropouts.append(dropout_fn(dropout))\n",
    "\n",
    "# Linear decoder\n",
    "decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Run model\n",
    "\n",
    "\"\"\"\n",
    "Input x is shape (B, L, d_input)\n",
    "\"\"\"\n",
    "x = encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "#x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "for layer, norm, dropout in zip(s4_layers, norms, dropouts):\n",
    "    # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "    z = x\n",
    "    if prenorm:\n",
    "        # Prenorm\n",
    "        z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "    # Apply S4 block: we ignore the state input and output\n",
    "    z, _ = layer(z)\n",
    "\n",
    "    # Dropout on the output of the S4 block\n",
    "    z = dropout(z)\n",
    "\n",
    "    # Residual connection\n",
    "    x = z + x\n",
    "\n",
    "    if not prenorm:\n",
    "        # Postnorm\n",
    "        x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "# x = x.transpose(-1, -2)\n",
    "\n",
    "# Pooling: average pooling over the sequence length\n",
    "x = x.mean(dim=1)\n",
    "\n",
    "# Decode the outputs\n",
    "x = decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "#return x\n",
    "torch.argmax(x)\n",
    "torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aabd5e8e672e6a",
   "metadata": {},
   "source": [
    "# Create S4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c13251c1d7f15b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from s4.models.s4.s4d import S4D\n",
    "\n",
    "\n",
    "# Dropout broke in PyTorch 1.11\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) == (1, 11):\n",
    "    print(\"WARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\")\n",
    "    dropout_fn = nn.Dropout\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 12):\n",
    "    dropout_fn = nn.Dropout1d\n",
    "else:\n",
    "    dropout_fn = nn.Dropout2d\n",
    "    \n",
    "\n",
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.2,\n",
    "        prenorm=False,\n",
    "        lr=1-2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (d_input = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, lr))\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(dropout_fn(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        \n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98dfb16954b11c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "model = S4Model(\n",
    "    d_input=d_input,\n",
    "    d_output=d_output,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    prenorm=prenorm,\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085a126464f2f70",
   "metadata": {},
   "source": [
    "# Setup Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fc219defba3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optimizer(model, lr, weight_decay, epochs):\n",
    "    \"\"\"\n",
    "    S4 requires a specific optimizer setup.\n",
    "\n",
    "    The S4 layer (A, B, C, dt) parameters typically\n",
    "    require a smaller learning rate (typically 0.001), with no weight decay.\n",
    "\n",
    "    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01)\n",
    "    and weight decay (if desired).\n",
    "    \"\"\"\n",
    "\n",
    "    # All parameters in the model\n",
    "    all_parameters = list(model.parameters())\n",
    "\n",
    "    # General parameters don't contain the special _optim key\n",
    "    params = [p for p in all_parameters if not hasattr(p, \"_optim\")]\n",
    "\n",
    "    # Create an optimizer with the general parameters\n",
    "    optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Add parameters with special hyperparameters\n",
    "    hps = [getattr(p, \"_optim\") for p in all_parameters if hasattr(p, \"_optim\")]\n",
    "    hps = [\n",
    "        dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n",
    "    ]  # Unique dicts\n",
    "    for hp in hps:\n",
    "        params = [p for p in all_parameters if getattr(p, \"_optim\", None) == hp]\n",
    "        optimizer.add_param_group(\n",
    "            {\"params\": params, **hp}\n",
    "        )\n",
    "\n",
    "    # Create a lr scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=0.2)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    # Print optimizer info\n",
    "    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        group_hps = {k: g.get(k, None) for k in keys}\n",
    "        print(' | '.join([\n",
    "            f\"Optimizer group {i}\",\n",
    "            f\"{len(g['params'])} tensors\",\n",
    "        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1364b2330406ab40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer group 0 | 28 tensors | lr 0.01 | weight_decay 0.01\n",
      "Optimizer group 1 | 12 tensors | lr 0.001 | weight_decay 0.0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer, scheduler = setup_optimizer(\n",
    "    model, lr=lr, weight_decay=weight_decay, epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3be9bfb598c1f",
   "metadata": {},
   "source": [
    "# Set train() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca5069882fb3d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs.shape)\n",
    "\n",
    "#         outputs_onehot = torch.zeros(d_output).to(device)\n",
    "#         print(outputs.shape)\n",
    "#         idx = torch.argmax(outputs,dim=1)\n",
    "#         #print(idx)\n",
    "#         outputs_onehot[idx] = 1\n",
    "#         outputs_onehot.to(device)\n",
    "#         print(outputs_onehot.shape)\n",
    "#         print(targets.shape)\n",
    "        \n",
    "        \n",
    "        predicted_indices = torch.argmax(outputs, dim=1).to(device)\n",
    "        one_hot_encoded = torch.nn.functional.one_hot(predicted_indices, num_classes=10).to(torch.float32).to(device)\n",
    "        #loss = criterion(one_hot_encoded, targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(0)\n",
    "        total += targets.size(0)\n",
    "        \n",
    "        #print(predicted)\n",
    "        \n",
    "        #print(predicted.eq(targets).sum().item())\n",
    "        \n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "            (batch_idx, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6737cf3b3852db6",
   "metadata": {},
   "source": [
    "# Set eval() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "337202de39ba4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(epoch, dataloader, checkpoint=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(dataloader))\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(torch.float32).to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "#             outputs_onehot = torch.zeros(d_output)\n",
    "#             idx = torch.argmax(outputs,dim=1)\n",
    "#             #print(idx)\n",
    "#             outputs_onehot[idx] = 1\n",
    "#             outputs_onehot.to(device)\n",
    "\n",
    "            predicted_indices = torch.argmax(outputs, dim=1).to(device)\n",
    "            one_hot_encoded = torch.nn.functional.one_hot(predicted_indices, num_classes=10).to(torch.float32).to(device)\n",
    "        \n",
    "            #loss = criterion(one_hot_encoded, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(0)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                (batch_idx, len(dataloader), eval_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "            )\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if checkpoint:\n",
    "        acc = 100.*correct/total\n",
    "        if acc > best_acc:\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(state, './checkpoint/ckpt.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4214238886351",
   "metadata": {},
   "source": [
    "# Run training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6816838ac1478db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9a3d62dc6342bd9ee9de845b0bf7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5611ce018ce44a1ac2a447c0e97a7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f08cb82fc74080a5119338c972aacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m | Val acc: \u001b[39m\u001b[38;5;132;01m%1.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, val_acc))\n\u001b[1;32m      7\u001b[0m train()\n\u001b[0;32m----> 8\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28meval\u001b[39m(epoch, testloader)\n\u001b[1;32m     10\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[62], line 28\u001b[0m, in \u001b[0;36meval\u001b[0;34m(epoch, dataloader, checkpoint)\u001b[0m\n\u001b[1;32m     26\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch Idx: (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) | Loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m | Acc: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     32\u001b[0m             (batch_idx, \u001b[38;5;28mlen\u001b[39m(dataloader), eval_loss\u001b[38;5;241m/\u001b[39m(batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m100.\u001b[39m\u001b[38;5;241m*\u001b[39mcorrect\u001b[38;5;241m/\u001b[39mtotal, correct, total)\n\u001b[1;32m     33\u001b[0m         )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Save checkpoint.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(start_epoch, epochs))\n",
    "for epoch in pbar:\n",
    "    if epoch == 0:\n",
    "        pbar.set_description('Epoch: %d' % (epoch))\n",
    "    else:\n",
    "        pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\n",
    "    train()\n",
    "    val_acc = eval(epoch, valloader, checkpoint=True)\n",
    "    eval(epoch, testloader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522c08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
